# 6.864 Advanced Natural Language Processing : Abstractive Summarization of Email Text

We demonstrate the use of Transfer Learning and Fine Tuning with state of the art pretrained transformer models, BERT and GPT2, for the task of Abstractive Summarization of Emails. There are two things that make this task difficult for NLP models. First, the text in emails is generally very different from the other corpora for summarization tasks, because it is partially conversational and often has a lot of acronyms, and there is much less data available for emails with annotated summaries. Second, for abstractive summarization the model needs to understand and retain the context of the email to generate a meaningful summary. Hence, we use pre-trained language models that have shown state of the art performance on several task and use transfer learning to improve them for email summarization. We compare our models with simple extractive summarization baselines and also show comparison between using transfer learning or applying the models directly; in each case we show that our approach beats the baselines even with limited training.
