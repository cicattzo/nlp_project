{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "5/20/21 - NLP Fine Tuned Summarization Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cicattzo/nlp_project/blob/main/5_20_21_NLP_Fine_Tuned_Summarization_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1sS8CzC9P_9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4hJGSvaDL6N"
      },
      "source": [
        "'''Define the model you want to run here:'''\n",
        "\n",
        "'''Wanted data sets the dataset to train on. The options are:\n",
        "cnn - full cnn dataset from online\n",
        "cnn_sample - cnn sample dataset from the dropbox\n",
        "bc3 - bc3 dataset from the dropbox\n",
        "merged_data - merged dataset from the dropbox'''\n",
        "wanted_data = 'merged_data'\n",
        "\n",
        "'''pretrained_model_name determines the pretrained model to load prior to training. The options are:\n",
        "bert - trains a bert-base-uncased to bert-base-uncased encoder decoder model\n",
        "gpt2 - trains a gp2 encoder decoder model\n",
        "pretrained_summarizer - pretrained summarization model on financial reports'''\n",
        "pretrained_model_name = 'bert-gpt2'\n",
        "\n",
        "'''Model type determines the architecture of the model to train on. The options are:\n",
        "original - fine tuned model with only a single linear layer\n",
        "bottleneck - bottleneck fine tuning with a linear layer scaling it down, dropout, then scaling it back up\n",
        "vanilla - no added layer'''\n",
        "model_type = 'original'\n",
        "\n",
        "'''Train the model or just load from memory and evaluate'''\n",
        "train_model = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__HCs3MArp2L"
      },
      "source": [
        "%%bash\n",
        "pip -q install torch\n",
        "pip -q install transformers\n",
        "# pip -q install datasets\n",
        "pip -q install tqdm\n",
        "# pip -q install rouge_score\n",
        "# pip -q install sacrebleu\n",
        "# pip install datasets==1.0.2\n",
        "# pip install sumy\n",
        "pip install SentencePiece\n",
        "# pip install transformers==4.0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yY7mMfIVh9a"
      },
      "source": [
        "%%bash\n",
        "# mkdir \"/content/gdrive/MyDrive/6864_project/\"\n",
        "cd \"/content/gdrive/MyDrive/6864_project/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h6gz7k09y2W"
      },
      "source": [
        "MODEL_FOLDER = \"/content/gdrive/MyDrive/6864_project/\"\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "# import datasets\n",
        "import transformers\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "# from sumy.parsers.plaintext import PlaintextParser\n",
        "# from sumy.nlp.tokenizers import Tokenizer\n",
        "# from sumy.summarizers.lsa import LsaSummarizer\n",
        "# from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "# from sumy.nlp.stemmers import Stemmer\n",
        "# from sumy.utils import get_stop_words\n",
        "from IPython.display import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDaBn89-ruff"
      },
      "source": [
        "#decide which dataset we want to train on \n",
        "if wanted_data == 'cnn':\n",
        "  train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:5%]\")\n",
        "  train_data = [x for x in train_data]\n",
        "  full_text_key = 'article'\n",
        "  label_key = 'highlights'\n",
        "elif wanted_data == 'bc3':\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "\n",
        "  #setting up to preprocess bc3 data\n",
        "  LANGUAGE = \"english\"\n",
        "  SENTENCES_COUNT = 10\n",
        "  tokenizer = Tokenizer(LANGUAGE)\n",
        "  stemmer = Stemmer(LANGUAGE)\n",
        "  \n",
        "  # change this line to any other summarizer\n",
        "  summarizer = TextRankSummarizer(stemmer)\n",
        "  summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "  #reading in and processing data\n",
        "  bc3_df = pd.read_csv(\"/content/gdrive/My Drive/6864_project/bc3_processed.csv\")\n",
        "  txt = bc3_df.iloc[0]['body']\n",
        "  summary = bc3_df.iloc[0]['summary']\n",
        "  parser = PlaintextParser.from_string(txt, tokenizer)\n",
        "  # keeping the subject and body separate, but they can be merged\n",
        "  bc3_df['unique_key'] = bc3_df['listno'] + \"-\" + bc3_df['email_num'].astype(str)\n",
        "  # train_data = bc3_df.groupby('unique_key').agg({'subject':lambda x: x.iloc[0], 'body':lambda x: x.iloc[0], 'summary':lambda x: x.to_list()}).to_dict('records')\n",
        "  train_data = bc3_df.agg({'subject':lambda x: x.iloc[0], 'body':lambda x: x.iloc[0], 'summary':lambda x: x}).to_dict('records')\n",
        "\n",
        "  test_data_pd = pd.read_csv(\"bc3_test.csv\")\n",
        "  test_data = test_data_pd.to_dict('records')\n",
        "  test_data = [x for x in test_data]\n",
        "\n",
        "  full_text_key = 'body'\n",
        "  label_key = 'summary'\n",
        "elif wanted_data == 'cnn_sample':\n",
        "  train_data = []\n",
        "  if train_model:\n",
        "    train_data_pd = pd.read_csv(\"cnn_train_data_5.csv\")\n",
        "    train_data = train_data_pd.to_dict('records')\n",
        "    train_data = [x for x in train_data]\n",
        "\n",
        "  test_data_pd = pd.read_csv(\"bc3_test.csv\")\n",
        "  test_data = test_data_pd.to_dict('records')\n",
        "  test_data = [x for x in test_data]\n",
        "\n",
        "  full_text_key = 'article'\n",
        "  label_key = 'highlights'\n",
        "\n",
        "elif wanted_data == 'merged_data':\n",
        "  train_data = []\n",
        "  if train_model:\n",
        "    train_data_pd = pd.read_csv(\"train_combined.csv\")\n",
        "    train_data = train_data_pd.to_dict('records')\n",
        "    train_data = [x for x in train_data]\n",
        "\n",
        "  test_data_pd = pd.read_csv(\"bc3_test.csv\")\n",
        "  test_data = test_data_pd.to_dict('records')\n",
        "  test_data = [x for x in test_data]\n",
        "\n",
        "  full_text_key = 'article'\n",
        "  label_key = 'highlights'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jT5ANJyBWDG"
      },
      "source": [
        "# print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xreA7ZYSrwIa"
      },
      "source": [
        "#intializing the tokenizer and choosing the pretrained model\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\n",
        "\n",
        "if pretrained_model_name == 'bert':\n",
        "  tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "  tokenizer2 = tokenizer\n",
        "if pretrained_model_name == 'bert-gpt2':\n",
        "  tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "  tokenizer2 = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "  tokenizer2.pad_token = tokenizer2.eos_token\n",
        "  # tokenizer2=tokenizer\n",
        "elif pretrained_model_name == 'gpt2':\n",
        "  tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer2 = tokenizer\n",
        "elif pretrained_model_name =='pretrained_summarizer':\n",
        "  model_name = \"human-centered-summarization/financial-summarization-pegasus\"\n",
        "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "  tokenizer2 = tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPGSwKNcr6g7"
      },
      "source": [
        "'''Defining our text summarization model as a class'''\n",
        "import torch.nn as nn\n",
        "device = 'cuda'\n",
        "encoder_max_length = 512\n",
        "decoder_max_length = 128\n",
        "\n",
        "class ModelOutputs:\n",
        "    def __init__(self, logits=None, loss=None):\n",
        "        self.logits = logits\n",
        "        self.loss = loss\n",
        "\n",
        "if model_type == 'original':\n",
        "  class TextSummarizationModel(nn.Module):\n",
        "\n",
        "      def __init__(self, lm=None, model_name=None):\n",
        "          '''\n",
        "          lm:         a pretrained transformer language model\n",
        "          dropout:    dropoutrate for the dropout layer\n",
        "          '''\n",
        "          super(TextSummarizationModel, self).__init__()\n",
        "          self.pretrained_model = lm\n",
        "          self.model_name = model_name\n",
        "          if model_name == 'pretrained_summarizer':\n",
        "            self.linear_layer = nn.Linear(lm.config.vocab_size, lm.config.vocab_size)\n",
        "          else:\n",
        "            self.linear_layer = nn.Linear(lm.config.decoder.hidden_size, tokenizer.vocab_size)\n",
        "      \n",
        "      def forward(self, input_ids=None, labels=None): #dont think I need anything besides input ids\n",
        "          if self.model_name == 'pretrained_summarizer':\n",
        "            outputs = self.pretrained_model(input_ids, max_length=decoder_max_length, num_beams=5, early_stopping=True, output_hidden_states=True)\n",
        "            logits = outputs.decoder_hidden_states[1]\n",
        "            new_logits = self.linear_layer(logits)  \n",
        "            final_logits = new_logits.permute(0,2,1)\n",
        "            \n",
        "            if labels is not None:\n",
        "              loss_fct = nn.NLLLoss(reduction=\"mean\").to(device)\n",
        "\n",
        "              loss = loss_fct(final_logits, labels)\n",
        "            else:\n",
        "              loss = 0\n",
        "          else:\n",
        "            outputs = self.pretrained_model(input_ids=input_ids, decoder_input_ids = labels, output_hidden_states=True)\n",
        "            logits = outputs.decoder_hidden_states[1]\n",
        "            # print(outputs.decoder_hidden_states[1].size())\n",
        "            # print(logits.size())\n",
        "            new_logits = self.linear_layer(logits)\n",
        "            final_logits = new_logits.permute(0,2,1)\n",
        "            \n",
        "            if labels is not None:\n",
        "              loss_fct = nn.NLLLoss(reduction=\"mean\").to(device)\n",
        "\n",
        "              loss = loss_fct(final_logits, labels)\n",
        "            else:\n",
        "              loss = 0\n",
        "          \n",
        "          return ModelOutputs(\n",
        "              logits=new_logits,\n",
        "              loss=loss)\n",
        "elif model_type == 'bottleneck':\n",
        "  class TextSummarizationModel(nn.Module):\n",
        "\n",
        "      def __init__(self, lm=None, model_name=None):\n",
        "          '''\n",
        "          lm:         a pretrained transformer language model\n",
        "          dropout:    dropoutrate for the dropout layer\n",
        "          '''\n",
        "          super(TextSummarizationModel, self).__init__()\n",
        "          self.pretrained_model = lm\n",
        "          self.model_name = model_name\n",
        "          self.dropout_layer = nn.Dropout(p=0.2)\n",
        "          if model_name == 'pretrained_summarizer':\n",
        "            self.bottleneck = nn.Linear(lm.config.vocab_size, int(lm.config.vocab_size * 0.5))\n",
        "            self.upscale = nn.Linear(int(lm.vocab_size * 0.5), lm.vocab_size)\n",
        "          else:\n",
        "            self.bottleneck = nn.Linear(lm.config.decoder.hidden_size, int(lm.config.decoder.hidden_size * 0.5))\n",
        "            self.upscale = nn.Linear(int(lm.config.decoder.hidden_size * 0.5), tokenizer.vocab_size)\n",
        "      \n",
        "      def forward(self, input_ids=None, labels=None): #dont think I need anything besides input ids\n",
        "          if self.model_name == 'pretrained_summarizer':\n",
        "            outputs = self.pretrained_model(input_ids, max_length=decoder_max_length, num_beams=5, early_stopping=True, output_hidden_states=True)\n",
        "            logits = outputs.decoder_hidden_states[1]\n",
        "            new_logits = self.bottleneck(logits)  \n",
        "            new_logits = self.dropout_layer(new_logits)\n",
        "            new_logits = self.upscale(new_logits)\n",
        "            final_logits = new_logits.permute(0,2,1)\n",
        "            \n",
        "            if labels is not None:\n",
        "              loss_fct = nn.NLLLoss(reduction=\"mean\").to(device)\n",
        "\n",
        "              loss = loss_fct(final_logits, labels)\n",
        "            else:\n",
        "              loss = 0\n",
        "          else:\n",
        "            outputs = self.pretrained_model(input_ids=input_ids, decoder_input_ids = labels, output_hidden_states=True)\n",
        "            logits = outputs.decoder_hidden_states[1]\n",
        "            new_logits = self.bottleneck(logits)  \n",
        "            new_logits = self.dropout_layer(new_logits)\n",
        "            new_logits = self.upscale(new_logits)\n",
        "            final_logits = new_logits.permute(0,2,1)\n",
        "            \n",
        "            if labels is not None:\n",
        "              loss_fct = nn.NLLLoss(reduction=\"mean\").to(device)\n",
        "\n",
        "              loss = loss_fct(final_logits, labels)\n",
        "            else:\n",
        "              loss = 0\n",
        "          \n",
        "          return ModelOutputs(\n",
        "              logits=new_logits,\n",
        "              loss=loss)\n",
        "elif model_type == 'vanilla':\n",
        "  class TextSummarizationModel(nn.Module):\n",
        "\n",
        "      def __init__(self, lm=None, model_name=None):\n",
        "          '''\n",
        "          lm:         a pretrained transformer language model\n",
        "          dropout:    dropoutrate for the dropout layer\n",
        "          '''\n",
        "          super(TextSummarizationModel, self).__init__()\n",
        "          self.pretrained_model = lm\n",
        "          self.model_name = model_name\n",
        "      \n",
        "      def forward(self, input_ids=None, labels=None): #dont think I need anything besides input ids\n",
        "          if self.model_name == 'pretrained_summarizer':\n",
        "            outputs = self.pretrained_model(input_ids, max_length=decoder_max_length, num_beams=5, early_stopping=True, output_hidden_states=True)\n",
        "            logits = outputs.last_hidden_state\n",
        "            final_logits = logits.permute(0,2,1)\n",
        "            \n",
        "            if labels is not None:\n",
        "              loss_fct = nn.NLLLoss(reduction=\"mean\").to(device)\n",
        "\n",
        "              loss = loss_fct(final_logits, labels)\n",
        "            else:\n",
        "              loss = 0\n",
        "          else:\n",
        "            outputs = self.pretrained_model(input_ids=input_ids, decoder_input_ids = labels, output_hidden_states=True)\n",
        "            logits = outputs.logits\n",
        "            final_logits = logits.permute(0,2,1)\n",
        "            \n",
        "            if labels is not None:\n",
        "              loss_fct = nn.NLLLoss(reduction=\"mean\").to(device)\n",
        "\n",
        "              loss = loss_fct(final_logits, labels)\n",
        "            else:\n",
        "              loss = 0\n",
        "          \n",
        "          return ModelOutputs(\n",
        "              logits=logits,\n",
        "              loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bHn3MS7MXa0"
      },
      "source": [
        "'''intializing our BERT pretrained encoder decoder model and using GPU'''\n",
        "\n",
        "# from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
        "from transformers import EncoderDecoderModel\n",
        "\n",
        "if pretrained_model_name == 'bert':\n",
        "  enc_dec_pretrained = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased')\n",
        "elif pretrained_model_name == 'bert-gpt2':\n",
        "  enc_dec_pretrained = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'gpt2')\n",
        "elif pretrained_model_name == 'gpt2':\n",
        "  enc_dec_pretrained = EncoderDecoderModel.from_encoder_decoder_pretrained('gpt2', 'gpt2')\n",
        "elif pretrained_model_name == 'pretrained_summarizer':\n",
        "  enc_dec_pretrained = PegasusForConditionalGeneration.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYf1tPiWhQqq"
      },
      "source": [
        "#defining it into our class\n",
        "model = TextSummarizationModel(enc_dec_pretrained, pretrained_model_name)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBC44oAYLLV3"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Hyper-parameters: you could try playing with different settings\n",
        "num_epochs = 1\n",
        "learning_rate = 3e-5\n",
        "weight_decay = 1e-5\n",
        "eps = 1e-6\n",
        "batch_size = 4 #was 32\n",
        "warmup_rate = 0.05\n",
        "\n",
        "# Calculating the number of warmup steps\n",
        "num_training_cases = len(train_data)\n",
        "t_total = (num_training_cases // batch_size + 1) * num_epochs\n",
        "ext_warmup_steps = int(warmup_rate * t_total)\n",
        "\n",
        "# Initializing an AdamW optimizer\n",
        "ext_optim = torch.optim.AdamW(model.parameters(), lr=learning_rate,\n",
        "                              eps=eps, weight_decay=weight_decay)\n",
        "\n",
        "# Initializing the learning rate scheduler [details are in the BERT paper]\n",
        "ext_sche = transformers.get_linear_schedule_with_warmup(\n",
        "    ext_optim, num_warmup_steps=ext_warmup_steps, num_training_steps=t_total\n",
        ")\n",
        "\n",
        "print(\"***** Training Info *****\")\n",
        "print(\"  Num examples = %d\" % t_total)\n",
        "print(\"  Num Epochs = %d\" % num_epochs)\n",
        "print(\"  Batch size = %d\" % batch_size)\n",
        "print(\"  Total optimization steps = %d\" % t_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOvRGAUDLufu"
      },
      "source": [
        "def gather_batch(batch, full_text_key, label_key):\n",
        "\n",
        "    # input_batch  = [x[full_text_key] for x in batch if len(x[full_text_key]) < encoder_max_length]\n",
        "    input_batch  = [x[full_text_key] if len(x[full_text_key]) < encoder_max_length else x[full_text_key][:encoder_max_length] for x in batch]\n",
        "    label_batch  = [x[label_key] for x in batch]\n",
        "\n",
        "    return input_batch, label_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2F9EKszcd02"
      },
      "source": [
        "def vectorize_batch(batch, tokenizer, full_text_key, label_key):\n",
        "    input_batch, label_batch = gather_batch(batch, full_text_key, label_key)\n",
        "\n",
        "    # Encode the main body\n",
        "    input_encode = tokenizer.batch_encode_plus(\n",
        "        input_batch,\n",
        "        max_length = encoder_max_length,\n",
        "        truncation = True,\n",
        "        padding = 'longest',\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "    input_ids = input_encode['input_ids'].to(device)\n",
        "\n",
        "    # Encode the summary\n",
        "    label_encode = tokenizer2.batch_encode_plus(\n",
        "        label_batch,\n",
        "        max_length = decoder_max_length,\n",
        "        truncation = True,\n",
        "        padding = 'max_length',\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "    label_ids = label_encode['input_ids'].to(device)\n",
        "\n",
        "    return input_ids, label_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehMkDafXMFlA"
      },
      "source": [
        "loss_lst = []\n",
        "\n",
        "if train_model:\n",
        "\n",
        "  model.train()\n",
        "  max_grad_norm = 1\n",
        "\n",
        "  print(\"Number of Epochs\", num_epochs)\n",
        "  tot_steps = num_training_cases / batch_size\n",
        "  print(\"Total Training Steps\", tot_steps)\n",
        "  step_id = 0\n",
        "  perc_steps = tot_steps * 0.05\n",
        "  cur_step_displayed = 0\n",
        "  for _ in range(num_epochs):\n",
        "\n",
        "      random.shuffle(train_data)\n",
        "\n",
        "      for i in tqdm(range(0, num_training_cases, batch_size), position=0, leave=True):\n",
        "          batch = train_data[i: i + batch_size]\n",
        "          input_ids, label_ids = vectorize_batch(batch, tokenizer, full_text_key, label_key)\n",
        "\n",
        "          model.zero_grad()\n",
        "\n",
        "          outputs = model(input_ids=input_ids, labels=label_ids)\n",
        "\n",
        "          # Back-propagate the loss signal and clip the gradients\n",
        "          loss = outputs.loss.mean()\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "          # Update neural network parameters and the learning rate\n",
        "          ext_optim.step()\n",
        "          ext_sche.step() # Update learning rate for better convergence\n",
        "\n",
        "          model.zero_grad()\n",
        "\n",
        "          if step_id >= cur_step_displayed + perc_steps:\n",
        "              loss_lst.append(loss)\n",
        "              print(f'\\tAt step {step_id}, the extraction loss = {loss}')\n",
        "              cur_step_displayed += perc_steps\n",
        "          \n",
        "          step_id += 1\n",
        "\n",
        "  torch.save(model.state_dict(), MODEL_FOLDER+\"/\" + \"text_summarization_model_{}_{}_{}.pt\".format(wanted_data, pretrained_model_name, model_type))\n",
        "  print('Finished Training')\n",
        "\n",
        "else:\n",
        "  # model.load_state_dict(torch.load(MODEL_FOLDER+\"/\" + \"text_summarization_model_{}_{}_{}.pt\".format(wanted_data, pretrained_model_name, model_type)))\n",
        "  model.load_state_dict(torch.load(MODEL_FOLDER + \"text_summarization_model_merged_data_bert-gpt2_original.pt\"))\n",
        "  model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z7pt9vOujKl"
      },
      "source": [
        "if train_model:\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "  f = plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Extraction Loss at Training Step\")\n",
        "  plt.xlabel(\"Step\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.plot(loss_lst)\n",
        "  plt.xticks(ticks=np.arange(0,len(loss_lst),10), labels=np.arange(0, len(loss_lst), 10)*100, rotation=30)\n",
        "  f.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrBh9GZ5YY2k"
      },
      "source": [
        "## Evaluating our model using ROUGE and BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0X3zuxeaBy-"
      },
      "source": [
        "%%bash\n",
        "pip install sacrebleu\n",
        "pip install rouge_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pfu5yx91AEY"
      },
      "source": [
        "PAD_INDEX = 0\n",
        "UNK_INDEX = 1\n",
        "SOS_INDEX = 2\n",
        "EOS_INDEX = 3\n",
        "def greedy_decode(batch_logits):\n",
        "  '''\n",
        "  decodes the logits in a greedy way, picks the most probable word till EOS token is found\n",
        "  logits: tensor, (batch_size x seq_len x vocab_size)\n",
        "  '''\n",
        "  batch_out_ids = torch.argmax(batch_logits, dim=2)\n",
        "  batch_predicted = []\n",
        "  for i in range(batch_out_ids.shape[0]):\n",
        "    out_ids = batch_out_ids[i] \n",
        "    out_ids_trunc = []\n",
        "    for id in out_ids:\n",
        "      if id == EOS_INDEX:\n",
        "        break\n",
        "      out_ids_trunc.append(id)\n",
        "    out_str = ' '.join(tokenizer2.batch_decode(torch.stack(out_ids_trunc)))\n",
        "    batch_predicted.append(out_str)\n",
        "  return batch_predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOxDqA0IYfEv"
      },
      "source": [
        "import sacrebleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "rscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POBZb6vCYfCU"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "#redefining the keys to the bc3 dataset in case that wasnt trained on\n",
        "full_text_key = 'body'\n",
        "label_key = 'summary'\n",
        "\n",
        "rouge_scores_list = []\n",
        "bleu_score_list = []\n",
        "predictions = []\n",
        "num_test_cases = len(test_data)\n",
        "\n",
        "for i in tqdm(range(0, num_test_cases, batch_size), position=0, leave=True):\n",
        "  batch = test_data[i: i + batch_size]\n",
        "  input_batch  = [x[full_text_key] if len(x[full_text_key]) < encoder_max_length else x[full_text_key][:encoder_max_length] for x in batch]\n",
        "  label_batch  = [x[label_key] for x in batch]\n",
        "\n",
        "  input_encode = tokenizer.batch_encode_plus(\n",
        "      input_batch,\n",
        "      max_length = encoder_max_length,\n",
        "      truncation = True,\n",
        "      padding = 'longest',\n",
        "      return_attention_mask = True,\n",
        "      return_tensors = 'pt'\n",
        "  )\n",
        "  input_ids = input_encode['input_ids'].to(device)\n",
        "\n",
        "  # Encode the summary\n",
        "  label_encode = tokenizer.batch_encode_plus(\n",
        "      label_batch,\n",
        "      max_length = decoder_max_length,\n",
        "      truncation = True,\n",
        "      padding = 'max_length',\n",
        "      return_attention_mask = True,\n",
        "      return_tensors = 'pt'\n",
        "  )\n",
        "  label_ids = label_encode['input_ids'].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids, labels=label_ids)\n",
        "\n",
        "  pred_batch = greedy_decode(outputs.logits)\n",
        "  predictions.extend(pred_batch)\n",
        "  rouge_scores_list.extend([rscorer.score(pred, targ) for targ, pred in zip(label_batch, pred_batch)])\n",
        "  bleu_score_list.extend([sacrebleu.raw_corpus_bleu([pred], [[targ]], .01).score for pred, targ in zip(pred_batch, label_batch)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZLR9pmaYe_g"
      },
      "source": [
        "print(np.mean([sc['rouge2'].fmeasure for sc in rouge_scores_list]))\n",
        "print(np.mean([sc['rouge1'].fmeasure for sc in rouge_scores_list]))\n",
        "print(np.mean(bleu_score_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94u6QMk-aR3G"
      },
      "source": [
        "predictions = pd.DataFrame({'predicted_summary': predictions})\n",
        "predictions.to_csv(MODEL_FOLDER+\"/\" + \"predictions_{}_{}_{}.csv\".format(wanted_data, pretrained_model_name, model_type))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdpO9HTrDkS6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}